# -*- coding: utf-8 -*-
"""
Vibereader LINE Webhook Bot (YouTube -> Transcript -> OpenAI Summary)

Features
- LINE webhook endpoint: POST /webhook
- Extract YouTube URL, fetch transcript, do Map-Reduce summarization
- Reply immediately then push final summary
- Background thread to avoid LINE webhook timeout
- Usage logging to /tmp/usage_log.jsonl
- Error logging to /tmp/error_log.txt
"""

import os
import re
import json
import time
import hmac
import base64
import hashlib
import traceback
import threading
from datetime import datetime
from typing import Optional, Tuple, Dict, Any, List

import requests
from flask import Flask, request, abort
from dotenv import load_dotenv
from youtube_transcript_api import YouTubeTranscriptApi
from openai import OpenAI
import random
import queue
from youtube_transcript_api._errors import YouTubeRequestFailed

load_dotenv()

# åŒä¸€æ™‚é–“æœ€å¤šå¹¾å€‹ transcript æŠ“å–ï¼ˆå»ºè­° 1~2ï¼‰
TRANSCRIPT_SEM = threading.Semaphore(int(os.getenv("TRANSCRIPT_CONCURRENCY", "1")))

# é‡å° YouTube transcript åšå¿«å–ï¼ˆè¨˜æ†¶é«”ç‰ˆï¼Œéƒ¨ç½²åœ¨å–®ä¸€ Render instance å¾ˆæœ‰ç”¨ï¼‰
TRANSCRIPT_CACHE: Dict[str, Tuple[float, str]] = {}  # video_id -> (ts, text)
CACHE_TTL_SEC = int(os.getenv("TRANSCRIPT_CACHE_TTL_SEC", "86400"))  # 1 day

# -----------------------------
# Background job queue (é¿å… thread æš´è¡)
# -----------------------------
JOB_QUEUE: "queue.Queue[dict]" = queue.Queue(maxsize=int(os.getenv("JOB_QUEUE_MAXSIZE", "200")))
WORKER_COUNT = int(os.getenv("WORKER_COUNT", "1"))  # å»ºè­° 1~2ï¼ˆå…ˆ 1 æœ€ç©©ï¼‰
WORKERS_STARTED = False
WORKERS_LOCK = threading.Lock()
# -----------------------------
# Load env
# -----------------------------


LINE_TOKEN = (os.getenv("LINE_CHANNEL_ACCESS_TOKEN") or "").strip()
LINE_CHANNEL_SECRET = (os.getenv("LINE_CHANNEL_SECRET") or "").strip()  # optional
OPENAI_API_KEY = (os.getenv("OPENAI_API_KEY") or "").strip()

if not LINE_TOKEN:
    raise RuntimeError("ç¼ºå°‘ LINE_CHANNEL_ACCESS_TOKENï¼Œè«‹åœ¨ç’°å¢ƒè®Šæ•¸æˆ– .env è¨­å®š")
if not OPENAI_API_KEY:
    raise RuntimeError("ç¼ºå°‘ OPENAI_API_KEYï¼Œè«‹åœ¨ç’°å¢ƒè®Šæ•¸æˆ– .env è¨­å®š")

client = OpenAI(api_key=OPENAI_API_KEY)

# Model
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini").strip()

# Where to log in container (Render)
USAGE_LOG_PATH = os.getenv("USAGE_LOG_PATH", "/tmp/usage_log.jsonl")
ERROR_LOG_PATH = os.getenv("ERROR_LOG_PATH", "/tmp/error_log.txt")


# -----------------------------
# Prompt templates
# -----------------------------
SUMMARY_TEMPLATE = """
ä½ æ˜¯ä¸€ä½è³‡æ·±ç¸½é«”ç¶“æ¿Ÿåˆ†æå¸«ã€‚è«‹æ ¹æ“šæä¾›çš„å­—å¹•å…§å®¹ï¼Œç”¨ç¹é«”ä¸­æ–‡ç”¢å‡ºã€Œ10 åˆ†é˜å…§è®€å®Œã€çš„å½±ç‰‡æ‘˜è¦ã€‚
è¦å‰‡ï¼š
- åš´ç¦æœæ’…ã€‚è‹¥å­—å¹•æœªæ˜ç¢ºæåˆ°ï¼Œè«‹å¯«ã€Œï¼ˆåŸæ–‡æœªæä¾›ç´°ç¯€ï¼‰ã€ã€‚
- æ‰€æœ‰æ•¸å­—ã€æ™‚é–“ã€ç™¾åˆ†æ¯”ã€æ”¿ç­–æ¢ä»¶ï¼Œå‹™å¿…ä¿ç•™ä¸¦æ”¾åœ¨ã€é—œéµæ•¸å­—èˆ‡å‡è¨­ã€‘ä¸­ã€‚
- åœ¨ã€å¸‚å ´å«ç¾©ã€‘ä¸­ç”¨ã€Œå› ç‚ºâ€¦æ‰€ä»¥â€¦ã€æè¿°å› æœéˆï¼Œä¸è¦åªä¸‹çµè«–ã€‚
- å…§å®¹ä»¥ã€Œç¸½ç¶“/åˆ©ç‡/é€šè†¨/å°±æ¥­/ç¾å…ƒ/é¢¨éšªè³‡ç”¢ã€è¦–è§’ç‚ºä¸»ã€‚

è¼¸å‡ºæ ¼å¼ï¼ˆè«‹åš´æ ¼ç…§é †åºï¼‰ï¼š
ã€ä¸‰è¡Œç¸½çµã€‘
- ...
- ...
- ...

ã€é‡é»æ¢åˆ—ã€‘
- ...ï¼ˆ8â€“12 é»ï¼Œæ¯é»æœ€å¤š 2 è¡Œï¼‰

ã€é—œéµæ•¸å­—èˆ‡å‡è¨­ã€‘
- æŒ‡æ¨™/æ•¸å­—ï¼š...ï¼ˆæ¢ä»¶ï¼š... / æ™‚é»ï¼š...ï¼‰

ã€å¸‚å ´å«ç¾©ã€‘
- è‚¡å¸‚ï¼š...
- å‚µå¸‚ï¼š...
- åŒ¯ç‡ï¼š...

ã€é¢¨éšªèˆ‡è¿½è¹¤æ¸…å–®ã€‘
- ...ï¼ˆ3â€“6 é»ï¼‰
""".strip()

MAP_INSTRUCTIONS = """
ä½ æ˜¯ç¸½é«”ç¶“æ¿Ÿåˆ†æå¸«åŠ©ç†ã€‚è«‹æŠŠé€™ä¸€æ®µå­—å¹•æç…‰æˆã€Œæ®µè½é‡é»ã€ï¼Œè¦æ±‚ï¼š
- åš´ç¦æœæ’…ï¼›ä¸ç¢ºå®šå°±å¯«ï¼ˆåŸæ–‡æœªæä¾›ç´°ç¯€ï¼‰
- åªä¿ç•™é€™æ®µå…§çœŸæ­£å‡ºç¾çš„è«–é»èˆ‡æ•¸å­—
- ä»¥æ¢åˆ—è¼¸å‡º 5â€“8 é»ï¼Œæ¯é»ä¸€å¥è©±ï¼Œç›¡é‡ä¿ç•™æ•¸å­—/æ¢ä»¶/æ™‚é–“
è¼¸å‡ºåªè¦æ¢åˆ—ï¼Œä¸è¦åŠ å…¶ä»–æ¨™é¡Œã€‚
""".strip()


# -----------------------------
# Flask app
# -----------------------------
app = Flask(__name__)


# -----------------------------
# Logging helpers
# -----------------------------
def _ts() -> str:
    return datetime.now().isoformat(timespec="seconds")


def log_error(where: str, err_text: str) -> None:
    """Log error to stdout + /tmp/error_log.txt (append)."""
    print(f"=== ERROR @ {where} ({_ts()}) ===")
    print(err_text)

    try:
        with open(ERROR_LOG_PATH, "a", encoding="utf-8") as f:
            f.write(f"\n[{_ts()}] {where}\n{err_text}\n")
    except Exception:
        # Even if file logging fails, stdout already has the traceback
        pass


def make_json_safe(obj: Any) -> Any:
    if obj is None:
        return None
    if isinstance(obj, (str, int, float, bool)):
        return obj
    if isinstance(obj, dict):
        return {k: make_json_safe(v) for k, v in obj.items()}
    if isinstance(obj, (list, tuple, set)):
        return [make_json_safe(x) for x in obj]
    if hasattr(obj, "__dict__"):
        return make_json_safe(obj.__dict__)
    return str(obj)


def log_usage(tag: str, usage: Dict[str, Any], extra: Dict[str, Any] = None) -> None:
    rec = {
        "ts": _ts(),
        "tag": tag,
        "model": OPENAI_MODEL,
        "usage": make_json_safe(usage),
    }
    if extra:
        rec["extra"] = make_json_safe(extra)

    try:
        with open(USAGE_LOG_PATH, "a", encoding="utf-8") as f:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    except Exception:
        # If file logging fails, don't break the workflow
        pass


# -----------------------------
# OpenAI helper
# -----------------------------
def openai_call(instructions: str, input_text: str) -> Tuple[str, Dict[str, Any]]:
    resp = client.responses.create(
        model=OPENAI_MODEL,
        instructions=instructions,
        input=input_text,
    )

    text = (getattr(resp, "output_text", "") or "").strip()

    usage = getattr(resp, "usage", None)
    if usage is None:
        usage_dict = {}
    elif isinstance(usage, dict):
        usage_dict = usage
    else:
        usage_dict = usage.__dict__ if hasattr(usage, "__dict__") else {"usage": str(usage)}

    return text, usage_dict


# -----------------------------
# LINE signature verification (optional)
# -----------------------------
def verify_line_signature(raw_body: bytes, signature_b64: str) -> bool:
    if not LINE_CHANNEL_SECRET:
        return True  # skip verification if not set

    mac = hmac.new(LINE_CHANNEL_SECRET.encode("utf-8"), raw_body, hashlib.sha256).digest()
    expected = base64.b64encode(mac).decode("utf-8")
    return hmac.compare_digest(expected, signature_b64 or "")


# -----------------------------
# YouTube helpers
# -----------------------------
def normalize_url_tail(url: str) -> str:
    # remove common trailing punctuations from chat
    return url.rstrip(")ã€‘]ã€‰ã€‹â€â€™\"'ã€‚ï¼Œ,.;:!ï¼ï¼Ÿ")


def extract_video_id(url: str) -> str:
    url = normalize_url_tail(url)

    # Common patterns:
    # - https://www.youtube.com/watch?v=VIDEOID
    # - https://youtu.be/VIDEOID
    # - https://www.youtube.com/shorts/VIDEOID
    patterns = [
        r"(?:v=)([A-Za-z0-9_\-]{11})",
        r"(?:youtu\.be/)([A-Za-z0-9_\-]{11})",
        r"(?:/shorts/)([A-Za-z0-9_\-]{11})",
    ]
    for p in patterns:
        m = re.search(p, url)
        if m:
            return m.group(1)

    raise ValueError("é€™ä¸æ˜¯æœ‰æ•ˆçš„ YouTube å½±ç‰‡é€£çµï¼ˆæ‰¾ä¸åˆ° videoIdï¼‰ï¼Œè«‹è²¼å®Œæ•´ç¶²å€")


def fetch_cc_transcript_text(video_id: str) -> str:
    # 1) cache hit
    now = time.time()
    cached = TRANSCRIPT_CACHE.get(video_id)
    if cached:
        ts, text = cached
        if now - ts < CACHE_TTL_SEC and text:
            return text

    # 2) concurrency guard
    with TRANSCRIPT_SEM:
        # Double-check cache (é¿å…ç­‰å¾… semaphore å¾Œåˆé‡æŠ“)
        cached = TRANSCRIPT_CACHE.get(video_id)
        if cached:
            ts, text = cached
            if time.time() - ts < CACHE_TTL_SEC and text:
                return text

        # 3) retry with exponential backoff + jitter (é‡å° 429 / sorry / 5xx ç‰¹åˆ¥æœ‰æ•ˆ)
        max_attempts = int(os.getenv("YT_TRANSCRIPT_MAX_ATTEMPTS", "6"))
        base_sleep = float(os.getenv("YT_TRANSCRIPT_BASE_SLEEP", "1.2"))
        last_err = None

        for attempt in range(1, max_attempts + 1):
            try:
                transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)

                preferred_langs = ["zh-Hant", "zh-TW", "zh-HK", "zh", "zh-Hans", "en"]
                transcript = None

                for lang in preferred_langs:
                    try:
                        transcript = transcript_list.find_manually_created_transcript([lang])
                        break
                    except Exception:
                        pass

                if transcript is None:
                    for lang in preferred_langs:
                        try:
                            transcript = transcript_list.find_generated_transcript([lang])
                            break
                        except Exception:
                            pass

                if transcript is None:
                    transcript = next(iter(transcript_list), None)
                    if transcript is None:
                        raise RuntimeError("No transcripts available")

                data = transcript.fetch()

                lines = []
                for item in data:
                    t = (item.get("text") or "").strip()
                    if t:
                        lines.append(t)
                text = "\n".join(lines)

                # cache store
                TRANSCRIPT_CACHE[video_id] = (time.time(), text)
                return text

            except Exception as e:
                last_err = e
                err_str = str(e).lower()

                # åˆ¤æ–·æ˜¯å¦æ˜¯ YouTube/Google é™æµé¡å‹ï¼ˆ429 / sorry / too many requestsï¼‰
                is_rate_limited = (
                    "too many requests" in err_str
                    or "google.com/sorry" in err_str
                    or "429" in err_str
                )

                # ä¸æ˜¯é™æµï¼šå°±ä¸è¦ä¸€ç›´é‡è©¦ï¼ˆä¾‹å¦‚å½±ç‰‡æ²’å­—å¹•ã€è¢«é—œé–‰ï¼‰
                if not is_rate_limited and attempt >= 2:
                    break

                # backoff with jitter
                sleep_s = min(60.0, base_sleep * (2 ** (attempt - 1)))
                sleep_s = sleep_s * (0.7 + random.random() * 0.6)  # 0.7~1.3x
                print(f"[yt_transcript] attempt {attempt}/{max_attempts} failed; sleep {sleep_s:.1f}s; err={e}")
                time.sleep(sleep_s)

        raise last_err if last_err else RuntimeError("Transcript fetch failed")




def compress_transcript(text: str, max_chars: int = 200000) -> str:
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    seen = set()
    uniq = []
    for ln in lines:
        if ln not in seen:
            seen.add(ln)
            uniq.append(ln)
    joined = "\n".join(uniq)
    return joined[:max_chars]


def chunk_text_by_chars(text: str, chunk_size: int = 4500, overlap: int = 350) -> List[str]:
    text = text.strip()
    if not text:
        return []
    chunks = []
    i = 0
    n = len(text)
    while i < n:
        j = min(n, i + chunk_size)
        chunks.append(text[i:j])
        if j == n:
            break
        i = max(0, j - overlap)
    return chunks


def openai_map_reduce_summary(transcript_text: str, youtube_url: str) -> str:
    cleaned = compress_transcript(transcript_text, max_chars=200000)
    chunks = chunk_text_by_chars(cleaned, chunk_size=4500, overlap=350)

    if not chunks:
        return "æŠ“ä¸åˆ°å­—å¹•å…§å®¹ï¼ˆå¯èƒ½å½±ç‰‡æœªæä¾›å­—å¹•æˆ–å­—å¹•å–å¾—å¤±æ•—ï¼‰ã€‚"

    partials: List[str] = []
    for idx, ch in enumerate(chunks, start=1):
        input_text = (
            f"ä¾†æºå½±ç‰‡ï¼š{youtube_url}\n"
            f"é€™æ˜¯å­—å¹•ç¬¬ {idx}/{len(chunks)} æ®µã€‚\n\n"
            f"{ch}"
        )
        part, usage = openai_call(MAP_INSTRUCTIONS, input_text)
        log_usage(f"map_{idx}", usage, extra={"chunks": len(chunks)})
        if part:
            partials.append(f"ã€æ®µè½ {idx}ã€‘\n{part}")

        time.sleep(0.15)

    if not partials:
        return "å­—å¹•æ‘˜è¦å¤±æ•—ï¼ˆæœªå–å¾—ä»»ä½•æ®µè½æ‘˜è¦ï¼‰ã€‚"

    reduce_input = "\n\n".join(partials)
    final_input = (
        f"ä¾†æºå½±ç‰‡ï¼š{youtube_url}\n"
        "ä»¥ä¸‹æ˜¯å„æ®µå­—å¹•çš„æ®µè½é‡é»ï¼ˆå·²åˆ†æ®µæ•´ç†ï¼‰ã€‚\n"
        "è«‹ä¾ç…§æ¨¡æ¿ç”¢å‡ºæœ€çµ‚æ‘˜è¦ã€‚\n\n"
        f"{reduce_input}"
    )

    final_text, usage = openai_call(SUMMARY_TEMPLATE, final_input)
    log_usage("reduce_final", usage, extra={"chunks": len(chunks)})

    return final_text.strip() or "æ‘˜è¦ç”¢ç”Ÿå¤±æ•—ï¼ˆæ¨¡å‹æœªè¼¸å‡ºå…§å®¹ï¼‰ã€‚"


# -----------------------------
# LINE Messaging API helpers
# -----------------------------
def line_reply(reply_token: str, text: str) -> None:
    url = "https://api.line.me/v2/bot/message/reply"
    headers = {
        "Authorization": f"Bearer {LINE_TOKEN}",
        "Content-Type": "application/json; charset=utf-8",
    }
    payload = {
        "replyToken": reply_token,
        "messages": [{"type": "text", "text": text[:4900]}],
    }
    r = requests.post(url, headers=headers, json=payload, timeout=20)
    r.raise_for_status()


def line_push(user_id: str, text: str) -> None:
    url = "https://api.line.me/v2/bot/message/push"
    headers = {
        "Authorization": f"Bearer {LINE_TOKEN}",
        "Content-Type": "application/json; charset=utf-8",
    }
    payload = {"to": user_id, "messages": [{"type": "text", "text": text[:4900]}]}
    r = requests.post(url, headers=headers, json=payload, timeout=30)
    r.raise_for_status()

def _worker_loop(worker_id: int) -> None:
    print(f"[worker-{worker_id}] started")
    while True:
        event = JOB_QUEUE.get()  # æœƒé˜»å¡ç­‰å¾…
        try:
            process_event(event)
        except Exception:
            log_error(f"worker_loop_{worker_id}", traceback.format_exc())
        finally:
            JOB_QUEUE.task_done()


def ensure_workers_started() -> None:
    global WORKERS_STARTED
    if WORKERS_STARTED:
        return
    with WORKERS_LOCK:
        if WORKERS_STARTED:
            return
        for wid in range(1, WORKER_COUNT + 1):
            t = threading.Thread(target=_worker_loop, args=(wid,), daemon=True)
            t.start()
        WORKERS_STARTED = True
        print(f"[queue] workers started: {WORKER_COUNT}")


# -----------------------------
# Background processing
# -----------------------------
def process_event(event: dict) -> None:
    """
    Heavy work in background:
    - Extract URL
    - Fetch transcript
    - Map-Reduce summarize
    - Push result
    """
    # æ–¹ä¾¿å®šä½ï¼šæ¯æ¬¡äº‹ä»¶æ‰“ä¸€å€‹çŸ­ id
    eid = (event.get("replyToken") or "")[:8] or "no_token"

    def _log(step: str, extra: str = ""):
        # Render Logs æœƒçœ‹åˆ°
        print(f"[{eid}] {step} {extra}".strip())

    try:
        reply_token = event.get("replyToken", "") or ""
        source = event.get("source", {}) or {}
        user_id = source.get("userId", "") or ""

        msg = event.get("message", {}) or {}
        user_text = (msg.get("text") or "").strip()
        _log("received", f"text_len={len(user_text)} user_id={'Y' if user_id else 'N'}")

        # Extract URL
        m = re.search(r"(https?://[^\s]+)", user_text)
        if not m:
            if reply_token:
                line_reply(reply_token, "è«‹è²¼ä¸Š YouTube å½±ç‰‡é€£çµï¼Œæˆ‘æœƒå¹«ä½ åšæ‘˜è¦ã€‚")
            _log("no_url")
            return

        youtube_url = normalize_url_tail(m.group(1))
        _log("url_extracted", youtube_url)

        # Reply ASAP (reply token only once)
        if reply_token:
            try:
                line_reply(reply_token, "æ”¶åˆ°é€£çµï¼Œé–‹å§‹æŠ“å­—å¹•ä¸¦æ•´ç†æ‘˜è¦ã€‚å®Œæˆå¾Œæˆ‘æœƒå†æŠŠæ‘˜è¦æ¨æ’­çµ¦ä½ ã€‚")
                _log("replied_ok")
            except Exception:
                _log("replied_failed")
                log_error("line_reply", traceback.format_exc())

        # Heavy work
        _log("extract_video_id")
        video_id = extract_video_id(youtube_url)
        _log("video_id", video_id)

        _log("fetch_transcript_start")
        try:
            transcript = fetch_cc_transcript_text(video_id)
        except Exception as e:
            # é‡å° YouTube transcript å¸¸è¦‹éŒ¯èª¤çµ¦æ›´å‹å–„çš„è¨Šæ¯
            err = traceback.format_exc()
            log_error("fetch_transcript", err)

            msg = "æŠ“ä¸åˆ°å­—å¹•ï¼ˆå¯èƒ½å½±ç‰‡æœªæä¾›å­—å¹•ã€å­—å¹•è¢«é—œé–‰ã€æˆ– YouTube æš«æ™‚æ‹’çµ•å­˜å–ï¼‰ã€‚ä½ å¯ä»¥æ›ä¸€æ”¯å½±ç‰‡æˆ–ç¨å¾Œå†è©¦ã€‚"
            if user_id:
                line_push(user_id, msg)
            _log("fetch_transcript_failed", str(e))
            return

        _log("fetch_transcript_ok", f"chars={len(transcript)}")

        _log("summarize_start")
        summary = openai_map_reduce_summary(transcript, youtube_url)
        _log("summarize_ok", f"chars={len(summary)}")

        # Push final summary
        if not user_id:
            log_error("process_event", "No userId found; cannot push summary.")
            _log("no_user_id")
            return

        # å¦‚æœæ‘˜è¦å¤ªé•·ï¼Œåˆ‡æ®µæ¨é€ï¼ˆé¿å…åªæ”¶åˆ°å‰ 4900ï¼‰
        parts = []
        s = summary.strip()
        while s:
            parts.append(s[:4900])
            s = s[4900:]

        _log("push_start", f"parts={len(parts)}")
        for i, part in enumerate(parts, start=1):
            line_push(user_id, part if len(parts) == 1 else f"({i}/{len(parts)})\n{part}")
            time.sleep(0.2)

        _log("push_ok")

    except Exception:
        err = traceback.format_exc()
        log_error("process_event", err)
        _log("fatal_error")

        # Best-effort push an error message
        try:
            user_id = (event.get("source", {}) or {}).get("userId", "") or ""
            if user_id:
                line_push(user_id, "æŠ±æ­‰ï¼Œè™•ç†æ‘˜è¦æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚æˆ‘å·²åœ¨ä¼ºæœå™¨ç«¯è¨˜éŒ„éŒ¯èª¤è¨Šæ¯ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–æ›ä¸€æ”¯å½±ç‰‡é€£çµã€‚")
                _log("pushed_error_msg")
        except Exception:
            log_error("line_push_error_fallback", traceback.format_exc())
            _log("push_error_msg_failed")



# -----------------------------
# Routes
# -----------------------------
@app.route("/healthz", methods=["GET"])
def healthz():
    return "OK", 200

@app.route("/webhook", methods=["POST"])
def webhook():
    raw_body = request.get_data()  # bytes
    signature = request.headers.get("X-Line-Signature", "")

    if not verify_line_signature(raw_body, signature):
        abort(400)

    body = request.get_json(silent=True)
    if not body:
        abort(400)

    try:
        ensure_workers_started()

        events = body.get("events", []) or []
        for event in events:
            if event.get("type") != "message":
                continue
            msg = event.get("message", {}) or {}
            if msg.get("type") != "text":
                continue

            try:
                JOB_QUEUE.put_nowait(event)
                print(f"[queue] enqueued event; qsize={JOB_QUEUE.qsize()}")
            except queue.Full:
                # Queue æ»¿äº†ï¼šçŸ­æ™‚é–“å¡å¤ªå¤š
                reply_token = event.get("replyToken", "") or ""
                if reply_token:
                    try:
                        line_reply(reply_token, "ç›®å‰åŒæ™‚è™•ç†çš„è«‹æ±‚è¼ƒå¤šï¼Œè«‹ç¨å¾Œå†è©¦ä¸€æ¬¡ ğŸ™")
                    except Exception:
                        log_error("line_reply_queue_full", traceback.format_exc())
                print("[queue] queue full; dropped event")

        return "OK", 200

    except Exception:
        log_error("webhook", traceback.format_exc())
        return "ERROR", 500


# -----------------------------
# Run locally
# -----------------------------
if __name__ == "__main__":
    # Local dev server
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", "8000")), debug=True)
